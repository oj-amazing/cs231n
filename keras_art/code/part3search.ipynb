{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import os\n",
    "import keras\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "use_full = False\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "#top_model_weights_path = '../model/bottleneck_fc_model.h5'\n",
    "top_model_weights_path = '../model/bottleneck_fc_model_inceptionv3_500_val_acc_4791.h5'\n",
    "#top_model_weights_path = '../model/fc_model.h5'\n",
    "img_width, img_height = 500, 500\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "image_train_file = '../data/image_train.npy'\n",
    "image_validation_file = '../data/image_validation.npy'\n",
    "image_test_file = '../data/image_test.npy'\n",
    "\n",
    "#full\n",
    "if use_full:\n",
    "    train_data_dir = '../data/Pandora18K_train_val_test_split/train'\n",
    "    validation_data_dir = '../data/Pandora18K_train_val_test_split/val'\n",
    "    test_data_dir = '../data/Pandora18K_train_val_test_split/test'\n",
    "    nb_train_samples = 14313\n",
    "    nb_validation_samples = 1772\n",
    "    nb_test_samples = 1791\n",
    "    total_num_classes = 18\n",
    "    validation_labels = np.array([0]*72+[1]*73+[2]*72+[3]*93+[4]*78+[5]*74+[6]*85+[7]*124+[8]*131+[9]*118+[10]*109+[11]*105+[12]*80+[13]*130+[14]*108+[15]*89+[16]*111+[17]*120)\n",
    "    train_labels = np.array([0]*684+[1]*598+[2]*655+[3]*657+[4]*808+[5]*675+[6]*715+[7]*946+[8]*995+[9]*1021+[10]*803+[11]*816+[12]*566+[13]*959+[14]*842+[15]*831+[16]*849+[17]*893)\n",
    "    test_labels = np.array([0]*91+[1]*60+[2]*75+[3]*82+[4]*104+[5]*83+[6]*95+[7]*121+[8]*131+[9]*123+[10]*103+[11]*117+[12]*65+[13]*123+[14]*121+[15]*112+[16]*89+[17]*96)\n",
    "else:\n",
    "    train_data_dir = '../data/Pandora18K_small_train_val_test_split/train'\n",
    "    validation_data_dir = '../data/Pandora18K_small_train_val_test_split/val'\n",
    "    test_data_dir = '../data/Pandora18K_small_train_val_test_split/test'\n",
    "    nb_train_samples = 1462\n",
    "    nb_validation_samples = 167\n",
    "    nb_test_samples = 171\n",
    "    total_num_classes = 18\n",
    "    validation_labels = np.array([0]*8+[1]*10+[2]*11+[3]*5+[4]*11+[5]*6+[6]*8+[7]*8+[8]*9+[9]*12+[10]*7+[11]*10+[12]*14+[13]*5+[14]*11+[15]*11+[16]*12+[17]*9)\n",
    "    train_labels = np.array([0]*78+[1]*79+[2]*77+[3]*85+[4]*78+[5]*87+[6]*80+[7]*81+[8]*81+[9]*80+[10]*85+[11]*83+[12]*74+[13]*87+[14]*83+[15]*83+[16]*78+[17]*83)\n",
    "    test_labels = np.array([0]*14+[1]*11+[2]*12+[3]*10+[4]*11+[5]*7+[6]*12+[7]*11+[8]*10+[9]*8+[10]*8+[11]*7+[12]*12+[13]*8+[14]*6+[15]*6+[16]*10+[17]*8)\n",
    "\n",
    "validation_labels = to_categorical(validation_labels, num_classes=18)\n",
    "train_labels = to_categorical(train_labels, num_classes=18)\n",
    "test_labels = to_categorical(test_labels, num_classes=18)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1462 images belonging to 18 classes.\n",
      "Found 167 images belonging to 18 classes.\n",
      "Found 171 images belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen  = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle = False)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle = False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1462 images belonging to 18 classes.\n",
      "23/23 [==============================] - 16s    \n",
      "(1462, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#Get the train image inputs as numpy arrays\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "model = Sequential()\n",
    "model.add(Dropout(0,input_shape = (img_width,img_height,3)))\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "img_train = model.predict_generator(\n",
    "    generator, (nb_train_samples // batch_size)+1, verbose=True, pickle_safe=True, workers=1)\n",
    "print(img_train.shape)\n",
    "del model\n",
    "np.save(open(image_train_file, 'w'), img_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 167 images belonging to 18 classes.\n",
      "3/3 [==============================] - 2s     \n",
      "(167, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#Get the validation image inputs as numpy arrays\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "model = Sequential()\n",
    "model.add(Dropout(0,input_shape = (img_width,img_height,3)))\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "img_validation = model.predict_generator(\n",
    "    generator, (nb_validation_samples // batch_size)+1, verbose=True, pickle_safe=True, workers=1)\n",
    "print(img_validation.shape)\n",
    "del model\n",
    "np.save(open(image_validation_file, 'w'), img_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171 images belonging to 18 classes.\n",
      "3/3 [==============================] - 2s     \n",
      "(171, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#Get the test image inputs as numpy arrays\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "model = Sequential()\n",
    "model.add(Dropout(0,input_shape = (img_width,img_height,3)))\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "img_test = model.predict_generator(\n",
    "    generator, (nb_test_samples // batch_size)+1, verbose=True, pickle_safe=True, workers=1)\n",
    "print(img_test.shape)\n",
    "del model\n",
    "np.save(open(image_test_file, 'w'), img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_data.shape\n",
      "(1462, 500, 500, 3)\n",
      "train_labels.shape\n",
      "(1462, 18)\n",
      "\n",
      "validation_data.shape\n",
      "(167, 500, 500, 3)\n",
      "validation_labels.shape\n",
      "(167, 18)\n",
      "\n",
      "test_data.shape\n",
      "(171, 500, 500, 3)\n",
      "test_labels.shape\n",
      "(171, 18)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(open(image_train_file))\n",
    "print(\"\\ntrain_data.shape\")\n",
    "print(train_data.shape)\n",
    "print(\"train_labels.shape\")\n",
    "print(train_labels.shape)\n",
    "\n",
    "validation_data = np.load(open(image_validation_file))\n",
    "print(\"\\nvalidation_data.shape\")\n",
    "print(validation_data.shape)\n",
    "print(\"validation_labels.shape\")\n",
    "print(validation_labels.shape)\n",
    "\n",
    "test_data = np.load(open(image_test_file))\n",
    "print(\"\\ntest_data.shape\")\n",
    "print(test_data.shape)\n",
    "print(\"test_labels.shape\")\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "lr = 1e-08\n",
      "momentum = 0.9\n",
      "number of frozen layers = 312\n",
      "Train on 1462 samples, validate on 167 samples\n",
      "Epoch 1/1\n",
      "1408/1462 [===========================>..] - ETA: 2s - loss: 5.0584 - acc: 0.1989Epoch 00000: val_acc improved from -inf to 0.80838, saving model to ../model/part_3_weights/lr_1e-08_best_weightspart_3_weights.h5\n",
      "1462/1462 [==============================] - 97s - loss: 5.0262 - acc: 0.2004 - val_loss: 0.8148 - val_acc: 0.8084\n",
      "167/167 [==============================] - 7s     \n",
      "new_acc = 0.808383236388\n",
      "new_loss = 0.814803152741\n",
      "best val acc so far = 0.808383236388\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = \"part_3_weights\"\n",
    "weight_dir = \"../model/%s\"%pretrained_model\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "\n",
    "best_weight_path = \"../model/best.h5\"\n",
    "best_acc = 0\n",
    "best_lr = 0\n",
    "best_momentum = 0\n",
    "best_frozen = 0\n",
    "num_parameter_sets = 1\n",
    "best_model = None\n",
    "    \n",
    "for i in range(num_parameter_sets):\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    lr = 10**-8#10**np.random.uniform(low=-7, high=-5)\n",
    "    momentum = 0.9#np.random.uniform(low = 0.8, high = 0.9)\n",
    "    num_frozen = 312#np.random.randint(300, high=312) #312 total\n",
    "    print(\"lr = \" + str(lr))\n",
    "    print(\"momentum = \" + str(momentum))\n",
    "    print(\"number of frozen layers = \" + str(num_frozen))\n",
    "    \n",
    "    # build the VGG16 network\n",
    "    base_model = applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(img_height,img_width,3))\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "    top_model.add(Dropout(0.6))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(.6))\n",
    "    top_model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "    top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "    model = Model(inputs= base_model.input, outputs= top_model(base_model.output))\n",
    "    \n",
    "    for layer in model.layers[:num_frozen]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[280:]:\n",
    "        layer.trainable = True\n",
    "    for layer in model.layers[311].layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=lr, momentum=momentum),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    param_names = \"lr_\" + str(lr)\n",
    "    weight_path = os.path.join(weight_dir, param_names + \"_best_weights%s.h5\"%pretrained_model)\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(filepath=weight_path, verbose=1,monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[checkpointer])\n",
    "    \n",
    "    \"\"\"\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=(nb_train_samples // batch_size)+1,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=(nb_validation_samples // batch_size)+1,\n",
    "        callbacks=[checkpointer, stopper])\n",
    "    \"\"\"\n",
    "    \n",
    "    model.load_weights(weight_path)\n",
    "    new_loss, new_acc = model.evaluate(validation_data, validation_labels, batch_size=batch_size, verbose=1, sample_weight=None)\n",
    "    #new_loss, new_acc = model.evaluate_generator(generator = validation_generator, steps = (nb_validation_samples // batch_size)+1)\n",
    "    print(\"new_acc = \"+ str(new_acc))\n",
    "    print(\"new_loss = \"+ str(new_loss))\n",
    "    \n",
    "    if new_acc > best_acc:\n",
    "        model.load_weights(weight_path)\n",
    "        model.save_weights(best_weight_path, overwrite = True)\n",
    "        best_acc = new_acc\n",
    "        best_lr = lr\n",
    "        best_momentum = momentum\n",
    "        best_frozen = num_frozen\n",
    "        best_model = model\n",
    "    #else:\n",
    "        #del model\n",
    "        \n",
    "    os.remove(weight_path)\n",
    "\n",
    "    \n",
    "    print(\"best val acc so far = \" + str(best_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 8s     \n",
      "best val loss = 0.814803152741\n",
      "best val acc = 0.808383236388\n"
     ]
    }
   ],
   "source": [
    "best_model.load_weights(best_weight_path)\n",
    "\n",
    "best_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=best_lr, momentum=best_momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "val_loss, val_acc = best_model.evaluate(validation_data, validation_labels, batch_size=batch_size, verbose=1, sample_weight=None)\n",
    "print(\"best val loss = \" + str(val_loss))\n",
    "print(\"best val acc = \" + str(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 11s    \n",
      "best test loss = 0.853165199882\n",
      "best test acc = 0.730994150304\n"
     ]
    }
   ],
   "source": [
    "best_model.load_weights(best_weight_path)\n",
    "\n",
    "best_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=best_lr, momentum=best_momentum),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(test_data, test_labels, batch_size=batch_size, verbose=1, sample_weight=None)\n",
    "print(\"best test loss = \" + str(test_loss))\n",
    "print(\"best test acc = \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#THIS IS NOT READY YET\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#y_test = to_categorical(validation_labels, num_classes=18)\n",
    "y_test = validation_labels\n",
    "prob_predict = model.predict_generator(generator = validation_generator, steps = (nb_validation_samples / batch_size) + 1, max_q_size=10, workers=1, pickle_safe=False, verbose=0)\n",
    "y_pred = np.argmax(prob_predict, axis=1)\n",
    "\n",
    "print(y_test.shape)\n",
    "print(prob_predict.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\n",
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import some data to play with\n",
    "\"\"\"\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = svm.SVC(kernel='linear', C=0.01)\n",
    "y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "\"\"\"\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "print(class_names)\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
