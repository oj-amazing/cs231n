{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_Pandora18K(data_dir = '../dataset/Pandora_18k_flat_and_resized'):\n",
    "    \n",
    "    i = 0\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        path = root.split(os.sep)\n",
    "        for file in files:\n",
    "            if '.jpg' in str(file).lower(): # an image file\n",
    "                file_path = os.path.join(root,file)\n",
    "\n",
    "                # Get image\n",
    "                imagedata = imread(file_path)\n",
    "                assert imagedata.shape == (500, 500, 3)\n",
    "                X_data.append(imagedata)\n",
    "                \n",
    "                # Get class\n",
    "                correct_class = -1\n",
    "                if \"01_Byzantin_Iconography\" in root:\n",
    "                    correct_class = 1\n",
    "                elif \"02_Early_Renaissance\" in root:\n",
    "                    correct_class = 2\n",
    "                elif \"03_Northern_Renaissance\" in root:\n",
    "                    correct_class = 3\n",
    "                elif \"04_High_Renaissance\" in root:\n",
    "                    correct_class = 4\n",
    "                elif \"05_Baroque\" in root:\n",
    "                    correct_class = 5\n",
    "                elif \"06_Rococo\" in root:\n",
    "                    correct_class = 6\n",
    "                elif \"07_Romanticism\" in root:\n",
    "                    correct_class = 7\n",
    "                elif \"08_Realism\" in root:\n",
    "                    correct_class = 8\n",
    "                elif \"09_Impressionism\" in root:\n",
    "                    correct_class = 9\n",
    "                elif \"10_Post_Impressionism\" in root:\n",
    "                    correct_class = 10\n",
    "                elif \"11_Expressionism\" in root:\n",
    "                    correct_class = 11\n",
    "                elif \"12_Symbolism\" in root:\n",
    "                    correct_class = 12\n",
    "                elif \"13_Fauvism\" in root:\n",
    "                    correct_class = 13\n",
    "                elif \"14_Cubism\" in root:\n",
    "                    correct_class = 14\n",
    "                elif \"15_Surrealism\" in root:\n",
    "                    correct_class = 15\n",
    "                elif \"16_AbstractArt\" in root:\n",
    "                    correct_class = 16\n",
    "                elif \"17_NaiveArt\" in root:\n",
    "                    correct_class = 17\n",
    "                elif \"18_PopArt\" in root:\n",
    "                    correct_class = 18\n",
    "                \n",
    "                assert correct_class != -1\n",
    "                Y_data.append(correct_class)\n",
    "                \n",
    "                \"\"\"\n",
    "                i = i + 1\n",
    "\n",
    "                if i >= 30: #this should be all of them, do the shuffling and subsampling below\n",
    "                    X_data_arr = np.array(X_data)\n",
    "                    Y_data_arr = np.array(Y_data)\n",
    "                    return X_data_arr, Y_data_arr\n",
    "                \"\"\"\n",
    "    X_data_arr = np.array(X_data)\n",
    "    Y_data_arr = np.array(Y_data)\n",
    "    return X_data_arr, Y_data_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pandora18k data...\n",
      "Shuffling Pandora18k data...\n",
      "[12 10  1  8 12  5  3 14  6 16]\n",
      "X_data.shape\n",
      "(17876, 500, 500, 3)\n",
      "Y_data.shape\n",
      "(17876,)\n"
     ]
    }
   ],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "data_dir = '../dataset/Pandora_18k_flat_and_resized'\n",
    "#17876 images total\n",
    "\n",
    "#grab data\n",
    "print(\"Loading Pandora18k data...\")\n",
    "X_data, Y_data = load_Pandora18K(data_dir)\n",
    "\n",
    "#shuffle x and y data together\n",
    "print(\"Shuffling Pandora18k data...\")\n",
    "X_data, Y_data = unison_shuffled_copies(X_data, Y_data)\n",
    "\n",
    "print(Y_data[0:10])\n",
    "\n",
    "print(\"X_data.shape\")\n",
    "print(X_data.shape)\n",
    "print(\"Y_data.shape\")\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_data()...\n",
      "Subsampling training...\n",
      "Subsampling validation...\n",
      "Subsampling test...\n",
      "Train data shape:  (1000, 500, 500, 3)\n",
      "Train labels shape:  (1000,)\n",
      "Validation data shape:  (500, 500, 500, 3)\n",
      "Validation labels shape:  (500,)\n",
      "Test data shape:  (500, 500, 500, 3)\n",
      "Test labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "#def get_data(num_training=8000, num_validation=1876, num_test=8000): \n",
    "def sub_sample_data(num_training=1000, num_validation=500, num_test=500): #test with 300 images total\n",
    "    \n",
    "    #training data subsample\n",
    "    print(\"Subsampling training...\")\n",
    "    mask = range(0, num_training)\n",
    "    X_train = X_data[mask]\n",
    "    y_train = Y_data[mask]\n",
    "    \n",
    "    #validation data subsample\n",
    "    print(\"Subsampling validation...\")\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_data[mask]\n",
    "    y_val = Y_data[mask]\n",
    "    \n",
    "    #test data subsample\n",
    "    print(\"Subsampling test...\")\n",
    "    mask = range(num_training + num_validation, num_training + num_validation + num_test)\n",
    "    X_test = X_data[mask]\n",
    "    y_test = Y_data[mask]\n",
    "    \n",
    "    \"\"\"\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    \n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \"\"\"\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "print(\"calling get_data()...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = sub_sample_data()\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d/Relu:0\", shape=(?, 50, 50, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 500, 500, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def style_model(img, y):\n",
    "    \"\"\"Generate class scores for img\n",
    "    \n",
    "    Inputs:\n",
    "    - img: Tensor of image [batch_size, 500, 500, 3]\n",
    "    - y: class [batch_size, 18]\n",
    "    \n",
    "    Returns:\n",
    "    Class scores [batch_size, 18].\n",
    "    \"\"\"\n",
    "    \n",
    "    dims = tf.shape(img)\n",
    "    batch_size = dims[0]\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=img,\n",
    "        filters=4,\n",
    "        kernel_size=[10, 10],\n",
    "        strides = [10,10],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    print(conv1)\n",
    "    \n",
    "    flat = tf.reshape(conv1, [batch_size, 10000]) #50 * 50 * 8\n",
    "    \n",
    "    dense = tf.layers.dense(inputs=flat, units=18)\n",
    "\n",
    "    return dense\n",
    "\n",
    "y_out = style_model(X, y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-18) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 49.6 and accuracy of 0.5\n",
      "Iteration 1: with minibatch training loss = 170 and accuracy of 0\n",
      "Iteration 2: with minibatch training loss = 114 and accuracy of 0\n",
      "Iteration 3: with minibatch training loss = 120 and accuracy of 0\n",
      "Iteration 4: with minibatch training loss = 27.1 and accuracy of 0\n",
      "Iteration 5: with minibatch training loss = 0.917 and accuracy of 1\n",
      "Iteration 6: with minibatch training loss = 5.65 and accuracy of 0\n",
      "Iteration 7: with minibatch training loss = 16.2 and accuracy of 0\n",
      "Iteration 8: with minibatch training loss = 3.47 and accuracy of 0\n",
      "Iteration 9: with minibatch training loss = 3.97 and accuracy of 0\n",
      "Iteration 10: with minibatch training loss = 3.95 and accuracy of 0\n",
      "Iteration 11: with minibatch training loss = 3.86 and accuracy of 0\n",
      "Iteration 12: with minibatch training loss = 2.62 and accuracy of 0.5\n",
      "Iteration 13: with minibatch training loss = 15.3 and accuracy of 0\n",
      "Iteration 14: with minibatch training loss = 2.95 and accuracy of 0\n",
      "Iteration 15: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 16: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 17: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 18: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 19: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 20: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 21: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 22: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 23: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 24: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 25: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 26: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 27: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 28: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 29: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 30: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 31: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 32: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 33: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 34: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 35: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 36: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 37: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 38: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 39: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 40: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 41: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 42: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 43: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 44: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 45: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 46: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 47: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 48: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 49: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 50: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 51: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 52: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 53: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 54: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 55: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 56: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 57: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 58: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 59: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 60: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 61: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 62: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 63: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 64: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 65: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 66: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 67: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 68: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 69: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 70: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 71: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 72: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 73: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 74: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 75: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 76: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 77: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 78: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 79: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 80: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 81: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 82: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 83: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 84: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 85: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 86: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 87: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 88: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 89: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 90: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 91: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 92: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 93: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 94: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 95: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 96: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 97: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 98: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 99: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 100: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 101: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 102: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 103: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 104: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 105: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 106: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 107: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 108: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 109: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 110: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 111: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 112: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 113: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 114: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 115: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 116: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 117: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 118: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 119: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 120: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 121: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 122: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 123: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 124: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 125: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 126: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 127: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 128: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 129: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 130: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 131: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 132: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 133: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 134: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 135: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 136: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 137: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 138: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 139: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 140: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 141: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 142: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 143: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 144: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 145: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 146: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 147: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 148: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 149: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 150: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 151: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 152: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 153: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 154: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 155: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 156: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 157: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 158: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 159: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 160: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 161: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 162: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 163: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 164: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 165: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 166: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 167: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 168: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 169: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 170: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 171: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 172: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 173: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 174: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 175: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 176: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 177: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 178: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 179: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 180: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 181: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 182: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 183: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 184: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 185: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 186: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 187: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 188: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 189: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 190: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 191: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 192: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 193: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 194: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 195: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 196: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 197: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 198: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 199: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 200: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 201: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 202: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 203: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 204: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 205: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 206: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 207: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 208: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 209: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 210: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 211: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 212: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 213: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 214: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 215: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 216: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 217: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 218: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 219: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 220: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 221: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 222: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 223: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 224: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 225: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 226: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 227: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 228: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 229: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 230: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 231: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 232: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 233: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 234: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 235: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 236: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 237: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 238: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 239: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 240: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 241: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 242: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 243: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 244: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 245: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 246: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 247: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 248: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 249: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 250: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 251: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 252: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 253: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 254: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 255: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 256: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 257: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 258: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 259: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 260: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 261: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 262: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 263: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 264: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 265: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 266: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 267: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 268: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 269: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 270: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 271: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 272: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 273: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 274: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 275: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 276: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 277: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 278: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 279: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 280: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 281: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 282: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 283: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 284: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 285: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 286: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 287: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 288: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 289: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 290: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 291: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 292: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 293: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 294: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 295: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 296: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 297: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 298: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 299: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 300: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 301: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 302: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 303: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 304: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 305: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 306: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 307: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 308: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 309: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 310: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 311: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 312: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 313: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 314: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 315: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 316: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 317: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 318: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 319: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 320: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 321: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 322: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 323: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 324: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 325: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 326: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 327: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 328: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 329: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 330: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 331: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 332: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 333: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 334: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 335: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 336: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 337: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 338: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 339: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 340: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 341: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 342: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 343: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 344: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 345: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 346: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 347: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 348: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 349: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 350: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 351: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 352: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 353: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 354: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 355: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 356: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 357: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 358: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 359: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 360: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 361: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 362: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 363: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 364: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 365: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 366: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 367: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 368: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 369: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 370: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 371: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 372: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 373: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 374: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 375: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 376: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 377: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 378: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 379: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 380: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 381: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 382: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 383: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 384: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 385: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 386: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 387: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 388: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 389: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 390: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 391: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 392: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 393: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 394: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 395: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 396: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 397: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 398: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 399: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 400: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 401: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 402: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 403: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 404: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 405: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 406: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 407: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 408: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 409: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 410: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 411: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 412: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 413: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 414: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 415: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 416: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 417: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 418: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 419: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 420: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 421: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 422: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 423: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 424: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 425: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 426: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 427: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 428: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 429: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 430: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 431: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 432: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 433: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 434: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 435: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 436: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 437: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 438: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 439: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 440: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 441: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 442: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 443: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 444: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 445: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 446: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 447: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 448: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 449: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 450: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 451: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 452: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 453: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 454: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 455: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 456: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 457: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 458: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 459: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 460: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 461: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 462: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 463: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 464: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 465: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 466: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 467: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 468: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 469: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 470: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 471: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 472: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 473: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 474: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 475: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 476: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 477: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 478: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 479: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 480: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 481: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 482: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 483: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 484: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 485: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 486: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 487: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 488: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 489: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 490: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 491: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 492: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 493: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 494: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 495: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 496: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 497: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 498: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 499: with minibatch training loss = nan and accuracy of 0\n",
      "Epoch 1, Overall loss = nan and accuracy of 0.004\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe871496c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 501: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 502: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 503: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 504: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 505: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 506: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 507: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 508: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 509: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 510: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 511: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 512: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 513: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 514: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 515: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 516: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 517: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 518: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 519: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 520: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 521: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 522: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 523: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 524: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 525: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 526: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 527: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 528: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 529: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 530: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 531: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 532: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 533: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 534: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 535: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 536: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 537: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 538: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 539: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 540: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 541: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 542: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 543: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 544: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 545: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 546: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 547: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 548: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 549: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 550: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 551: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 552: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 553: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 554: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 555: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 556: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 557: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 558: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 559: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 560: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 561: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 562: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 563: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 564: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 565: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 566: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 567: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 568: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 569: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 570: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 571: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 572: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 573: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 574: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 575: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 576: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 577: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 578: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 579: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 580: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 581: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 582: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 583: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 584: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 585: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 586: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 587: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 588: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 589: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 590: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 591: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 592: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 593: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 594: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 595: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 596: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 597: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 598: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 599: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 600: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 601: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 602: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 603: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 604: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 605: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 606: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 607: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 608: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 609: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 610: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 611: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 612: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 613: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 614: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 615: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 616: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 617: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 618: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 619: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 620: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 621: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 622: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 623: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 624: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 625: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 626: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 627: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 628: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 629: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 630: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 631: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 632: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 633: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 634: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 635: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 636: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 637: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 638: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 639: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 640: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 641: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 642: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 643: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 644: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 645: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 646: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 647: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 648: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 649: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 650: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 651: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 652: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 653: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 654: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 655: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 656: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 657: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 658: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 659: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 660: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 661: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 662: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 663: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 664: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 665: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 666: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 667: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 668: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 669: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 670: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 671: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 672: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 673: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 674: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 675: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 676: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 677: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 678: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 679: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 680: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 681: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 682: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 683: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 684: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 685: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 686: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 687: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 688: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 689: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 690: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 691: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 692: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 693: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 694: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 695: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 696: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 697: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 698: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 699: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 700: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 701: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 702: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 703: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 704: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 705: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 706: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 707: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 708: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 709: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 710: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 711: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 712: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 713: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 714: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 715: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 716: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 717: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 718: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 719: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 720: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 721: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 722: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 723: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 724: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 725: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 726: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 727: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 728: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 729: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 730: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 731: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 732: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 733: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 734: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 735: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 736: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 737: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 738: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 739: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 740: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 741: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 742: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 743: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 744: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 745: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 746: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 747: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 748: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 749: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 750: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 751: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 752: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 753: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 754: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 755: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 756: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 757: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 758: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 759: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 760: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 761: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 762: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 763: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 764: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 765: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 766: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 767: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 768: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 769: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 770: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 771: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 772: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 773: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 774: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 775: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 776: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 777: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 778: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 779: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 780: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 781: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 782: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 783: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 784: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 785: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 786: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 787: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 788: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 789: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 790: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 791: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 792: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 793: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 794: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 795: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 796: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 797: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 798: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 799: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 800: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 801: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 802: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 803: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 804: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 805: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 806: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 807: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 808: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 809: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 810: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 811: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 812: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 813: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 814: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 815: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 816: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 817: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 818: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 819: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 820: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 821: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 822: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 823: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 824: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 825: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 826: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 827: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 828: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 829: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 830: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 831: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 832: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 833: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 834: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 835: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 836: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 837: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 838: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 839: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 840: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 841: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 842: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 843: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 844: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 845: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 846: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 847: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 848: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 849: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 850: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 851: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 852: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 853: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 854: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 855: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 856: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 857: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 858: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 859: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 860: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 861: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 862: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 863: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 864: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 865: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 866: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 867: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 868: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 869: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 870: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 871: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 872: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 873: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 874: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 875: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 876: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 877: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 878: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 879: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 880: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 881: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 882: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 883: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 884: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 885: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 886: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 887: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 888: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 889: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 890: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 891: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 892: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 893: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 894: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 895: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 896: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 897: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 898: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 899: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 900: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 901: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 902: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 903: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 904: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 905: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 906: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 907: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 908: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 909: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 910: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 911: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 912: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 913: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 914: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 915: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 916: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 917: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 918: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 919: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 920: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 921: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 922: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 923: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 924: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 925: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 926: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 927: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 928: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 929: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 930: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 931: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 932: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 933: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 934: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 935: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 936: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 937: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 938: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 939: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 940: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 941: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 942: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 943: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 944: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 945: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 946: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 947: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 948: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 949: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 950: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 951: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 952: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 953: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 954: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 955: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 956: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 957: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 958: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 959: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 960: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 961: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 962: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 963: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 964: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 965: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 966: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 967: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 968: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 969: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 970: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 971: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 972: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 973: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 974: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 975: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 976: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 977: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 978: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 979: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 980: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 981: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 982: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 983: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 984: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 985: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 986: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 987: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 988: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 989: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 990: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 991: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 992: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 993: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 994: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 995: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 996: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 997: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 998: with minibatch training loss = nan and accuracy of 0\n",
      "Iteration 999: with minibatch training loss = nan and accuracy of 0\n",
      "Epoch 2, Overall loss = nan and accuracy of 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe871480f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = nan and accuracy of 0\n",
      "Test\n",
      "Epoch 1, Overall loss = nan and accuracy of 0\n"
     ]
    }
   ],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs, batch_size, print_every=5,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "my_batch_size = 2\n",
    "train_num_epochs = 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/gpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,train_num_epochs,my_batch_size,1,train_step,True)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,mean_loss,X_val,y_val,1,my_batch_size)\n",
    "        print('Test')\n",
    "        run_model(sess,y_out,mean_loss,X_test,y_test,1,my_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
