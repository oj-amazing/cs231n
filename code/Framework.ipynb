{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_Pandora18K(data_dir = '../dataset/Pandora_18k_flat_and_resized'):\n",
    "    \n",
    "    i = 0\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        path = root.split(os.sep)\n",
    "        for file in files:\n",
    "            if '.jpg' in str(file).lower(): # an image file\n",
    "                file_path = os.path.join(root,file)\n",
    "\n",
    "                # Get image\n",
    "                imagedata = imread(file_path)\n",
    "                assert imagedata.shape == (500, 500, 3)\n",
    "                X_data.append(imagedata)\n",
    "                \n",
    "                # Get class\n",
    "                correct_class = -1\n",
    "                if \"01_Byzantin_Iconography\" in root:\n",
    "                    correct_class = 1\n",
    "                elif \"02_Early_Renaissance\" in root:\n",
    "                    correct_class = 2\n",
    "                elif \"03_Northern_Renaissance\" in root:\n",
    "                    correct_class = 3\n",
    "                elif \"04_High_Renaissance\" in root:\n",
    "                    correct_class = 4\n",
    "                elif \"05_Baroque\" in root:\n",
    "                    correct_class = 5\n",
    "                elif \"06_Rococo\" in root:\n",
    "                    correct_class = 6\n",
    "                elif \"07_Romanticism\" in root:\n",
    "                    correct_class = 7\n",
    "                elif \"08_Realism\" in root:\n",
    "                    correct_class = 8\n",
    "                elif \"09_Impressionism\" in root:\n",
    "                    correct_class = 9\n",
    "                elif \"10_Post_Impressionism\" in root:\n",
    "                    correct_class = 10\n",
    "                elif \"11_Expressionism\" in root:\n",
    "                    correct_class = 11\n",
    "                elif \"12_Symbolism\" in root:\n",
    "                    correct_class = 12\n",
    "                elif \"13_Fauvism\" in root:\n",
    "                    correct_class = 13\n",
    "                elif \"14_Cubism\" in root:\n",
    "                    correct_class = 14\n",
    "                elif \"15_Surrealism\" in root:\n",
    "                    correct_class = 15\n",
    "                elif \"16_AbstractArt\" in root:\n",
    "                    correct_class = 16\n",
    "                elif \"17_NaiveArt\" in root:\n",
    "                    correct_class = 17\n",
    "                elif \"18_PopArt\" in root:\n",
    "                    correct_class = 18\n",
    "                \n",
    "                assert correct_class != -1\n",
    "                Y_data.append(correct_class)\n",
    "                \n",
    "                \"\"\"\n",
    "                i = i + 1\n",
    "\n",
    "                if i >= 30: #this should be all of them, do the shuffling and subsampling below\n",
    "                    X_data_arr = np.array(X_data)\n",
    "                    Y_data_arr = np.array(Y_data)\n",
    "                    return X_data_arr, Y_data_arr\n",
    "                \"\"\"\n",
    "    X_data_arr = np.array(X_data)\n",
    "    Y_data_arr = np.array(Y_data)\n",
    "    return X_data_arr, Y_data_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pandora18k data...\n",
      "Shuffling Pandora18k data...\n",
      "[ 8  5  9 16  5  6  8 18 15  9]\n",
      "X_data.shape\n",
      "(17876, 500, 500, 3)\n",
      "Y_data.shape\n",
      "(17876,)\n"
     ]
    }
   ],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "data_dir = '../dataset/Pandora_18k_flat_and_resized'\n",
    "#17876 images total\n",
    "\n",
    "#grab data\n",
    "print(\"Loading Pandora18k data...\")\n",
    "X_data, Y_data = load_Pandora18K(data_dir)\n",
    "\n",
    "#shuffle x and y data together\n",
    "print(\"Shuffling Pandora18k data...\")\n",
    "X_data, Y_data = unison_shuffled_copies(X_data, Y_data)\n",
    "\n",
    "print(Y_data[0:10])\n",
    "\n",
    "print(\"X_data.shape\")\n",
    "print(X_data.shape)\n",
    "print(\"Y_data.shape\")\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_data()...\n",
      "Subsampling training...\n",
      "Subsampling validation...\n",
      "Subsampling test...\n",
      "X_train data type\n",
      "float64\n",
      "X_val data type\n",
      "float64\n",
      "X_test data type\n",
      "float64\n",
      "Train data shape:  (500, 500, 500, 3)\n",
      "Train labels shape:  (500,)\n",
      "Validation data shape:  (500, 500, 500, 3)\n",
      "Validation labels shape:  (500,)\n",
      "Test data shape:  (500, 500, 500, 3)\n",
      "Test labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "#def get_data(num_training=8000, num_validation=1876, num_test=8000): \n",
    "def sub_sample_data(num_training=500, num_validation=500, num_test=500): #test with 300 images total\n",
    "    \n",
    "    #training data subsample\n",
    "    print(\"Subsampling training...\")\n",
    "    mask = range(0, num_training)\n",
    "    X_train = X_data[mask]\n",
    "    y_train = Y_data[mask]\n",
    "    \n",
    "    #validation data subsample\n",
    "    print(\"Subsampling validation...\")\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_data[mask]\n",
    "    y_val = Y_data[mask]\n",
    "    \n",
    "    #test data subsample\n",
    "    print(\"Subsampling test...\")\n",
    "    mask = range(num_training + num_validation, num_training + num_validation + num_test)\n",
    "    X_test = X_data[mask]\n",
    "    y_test = Y_data[mask]\n",
    "    \n",
    "    \"\"\"\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    \n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \"\"\"\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "print(\"calling get_data()...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = sub_sample_data()\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "X_val = X_val.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "print(\"X_train data type\")\n",
    "print(X_train.dtype)\n",
    "print(\"X_val data type\")\n",
    "print(X_val.dtype)\n",
    "print(\"X_test data type\")\n",
    "print(X_test.dtype)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2\n",
      "Tensor(\"conv2d_2/Relu:0\", shape=(?, 5, 5, 4), dtype=float32)\n",
      "\n",
      "labels shape\n",
      "Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int64)\n",
      "\n",
      "logits shape\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 18), dtype=float32)\n",
      "\n",
      "printing total_loss_array\n",
      "Tensor(\"clip_by_value:0\", shape=(?,), dtype=float32)\n",
      "\n",
      "printing mean_loss\n",
      "Tensor(\"clip_by_value_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 500, 500, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def style_model(img, y):\n",
    "    \"\"\"Generate class scores for img\n",
    "    \n",
    "    Inputs:\n",
    "    - img: Tensor of image [batch_size, 500, 500, 3]\n",
    "    - y: class [batch_size, 18]\n",
    "    \n",
    "    Returns:\n",
    "    Class scores [batch_size, 18].\n",
    "    \"\"\"\n",
    "    \n",
    "    dims = tf.shape(img)\n",
    "    batch_size = dims[0]\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=img,\n",
    "        filters=4,\n",
    "        kernel_size=[10, 10],\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        strides = [10,10],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    tf.verify_tensor_all_finite(conv1, \"conv1 not ok\", name=None)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=conv1,\n",
    "        filters=4,\n",
    "        kernel_size=[10, 10],\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        strides = [10,10],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    print(\"conv2\")\n",
    "    print(conv2)\n",
    "    \n",
    "    tf.verify_tensor_all_finite(conv2, \"conv2 not ok\", name=None)\n",
    "    \n",
    "    flat = tf.reshape(conv2, [batch_size, 100]) #5 * 5 * 4\n",
    "    \n",
    "    tf.verify_tensor_all_finite(flat, \"flat not ok\", name=None)\n",
    "    \n",
    "    dense = tf.layers.dense(\n",
    "        inputs=flat, \n",
    "        units=18,\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "        bias_initializer=tf.zeros_initializer())\n",
    "\n",
    "    tf.verify_tensor_all_finite(dense, \"dense not ok\", name=None)  \n",
    "    \n",
    "    return dense\n",
    "\n",
    "y_out = style_model(X, y)\n",
    "\n",
    "print(\"\\nlabels shape\")\n",
    "print(y)\n",
    "print(\"\\nlogits shape\")\n",
    "print(y_out)\n",
    "\n",
    "# define our loss\n",
    "temp = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_out)\n",
    "#total_loss_array = temp\n",
    "total_loss_array = tf.clip_by_value(temp, 0, 1000)\n",
    "print(\"\\nprinting total_loss_array\")\n",
    "print(total_loss_array)\n",
    "tf.verify_tensor_all_finite(total_loss_array, \"total_loss_array not ok\", name=None)\n",
    "\n",
    "\n",
    "mean_loss = tf.clip_by_value(tf.reduce_mean(total_loss_array), -1e-10, 1e10)\n",
    "print(\"\\nprinting mean_loss\")\n",
    "print(mean_loss)\n",
    "tf.verify_tensor_all_finite(mean_loss, \"mean_loss not ok\", name=None)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-40,#5e-28,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1,#1e-07,\n",
    "    use_locking=False,\n",
    "    name='Adam')\n",
    "#optimizer = tf.train.AdamOptimizer(5e-18) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "\n",
    "#g_step = tf.Variable(0, trainable=False)\n",
    "#learning_rate = 5e-18#tf.train.exponential_decay(5e-18, g_step,int(40000)/2, self.config.flag.step_decay_rate, staircase=True)\n",
    "#optimizer = tf.train.AdamOptimizer(5e-18)\n",
    "#tuples = optimizer.compute_gradients(mean_loss)\n",
    "#grads = [entry[0] for entry in tuples]\n",
    "#vars = [entry[1] for entry in tuples]\n",
    "#outGrad = tf.global_norm(grads)        # FOR DEBUGGING\n",
    "#grads, _ = tf.clip_by_global_norm(grads, 10)\n",
    "#clipped_gradients = zip(grads, vars)\n",
    "#learning_rate = learning_rate          # FOR DEBUGGING\n",
    "#train_step = optimizer.apply_gradients(clipped_gradients, global_step=g_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "\n",
      "t_loss\n",
      "(100,)\n",
      "[  1.92498760e+01   7.06151581e+01   2.68080101e+01   4.69657135e+01\n",
      "   3.07054043e+01   2.05714512e+01   6.85503540e+01   5.50633698e+01\n",
      "   1.37040787e+01   4.93086243e+00   7.08369446e+01   5.64337730e+01\n",
      "   7.09975662e+01   7.71626816e+01   3.68283195e+01   1.04930458e+02\n",
      "   7.72435608e+01   5.07127075e+01   1.87121830e+01   5.79791565e+01\n",
      "   3.92964058e+01   3.18094158e+01   3.22057190e+01   5.10487518e+01\n",
      "   1.20866556e+01   3.08193398e+01   4.92256403e+00   3.03979068e-05\n",
      "   3.20818634e+01   1.01524986e+02   2.35551739e+01   1.99687672e+00\n",
      "   9.97556782e+00   1.19209282e-07   7.84195280e+00   5.56348963e-03\n",
      "   8.25136948e+01   6.07739220e+01   1.18566427e+01   6.67801437e+01\n",
      "   3.98052483e+01   6.88585663e+01   6.90593719e-02   1.00000000e+03\n",
      "   8.34774399e+01   3.28486023e+01   3.58712158e+01   5.02100372e+01\n",
      "   1.66773815e+01   1.00000000e+03   1.36554127e+01   1.49006348e+01\n",
      "   1.82581064e-03   4.50665436e+01   1.04851074e+02   1.00000000e+03\n",
      "   2.31579514e+01   1.00000000e+03   0.00000000e+00   6.29410315e+00\n",
      "   2.55345116e+01   3.05585899e+01   3.04399681e+01   2.96038647e+01\n",
      "   4.36490326e+01   1.00000000e+03   4.45000954e+01   1.31185898e+02\n",
      "   8.33731308e+01   2.63899517e+01   5.32283630e+01   1.21959162e+01\n",
      "   2.49104500e+01   4.76592751e+01   1.00000000e+03   2.18725586e+01\n",
      "   1.74626656e+01   5.87019234e+01   4.06971092e+01   4.49263039e+01\n",
      "   1.66735668e+01   8.53471756e+01   1.48797467e-01   7.06382990e+00\n",
      "   9.89448395e+01   4.00231094e+01   4.44551849e+01   3.81711781e-01\n",
      "   1.14393417e+02   1.00000000e+03   2.73475266e+01   5.07447357e+01\n",
      "   1.02892952e+02   5.68204460e+01   7.89185524e+00   5.35422173e+01\n",
      "   5.71397629e+01   3.52212410e+01   3.70950050e+01   1.05890827e+01]\n",
      "corr\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False  True False  True\n",
      " False False False False False False  True False False False False False\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False]\n",
      "Iteration 0: with minibatch training loss = 107 and accuracy of 0.08\n",
      "\n",
      "\n",
      "t_loss\n",
      "(100,)\n",
      "[ 1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.]\n",
      "corr\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "Iteration 1: with minibatch training loss = 1e+03 and accuracy of 0\n",
      "\n",
      "\n",
      "t_loss\n",
      "(100,)\n",
      "[ 1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.]\n",
      "corr\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "Iteration 2: with minibatch training loss = 1e+03 and accuracy of 0\n",
      "\n",
      "\n",
      "t_loss\n",
      "(100,)\n",
      "[ 1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.]\n",
      "corr\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "Iteration 3: with minibatch training loss = 1e+03 and accuracy of 0\n",
      "\n",
      "\n",
      "t_loss\n",
      "(100,)\n",
      "[ 1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.\n",
      "  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.  1000.]\n",
      "corr\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "Iteration 4: with minibatch training loss = 1e+03 and accuracy of 0\n",
      "Epoch 1, Overall loss = 821 and accuracy of 0.016\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNmZmljsnGzMzy52TjZmZ5e7/AyzRCFFTiQrTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cea83cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs, batch_size, print_every=1,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = True#training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,total_loss_array,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            \n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            \n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "\n",
    "            print(\"\\n\")\n",
    "            if (np.isnan(Xd).any()):\n",
    "                print(\"error, there is NAN in Xd\")\n",
    "            if (np.isnan(yd).any()):\n",
    "                print(\"error, there is NAN in yd\")\n",
    "            \n",
    "\n",
    "            \"\"\"\n",
    "            print(\"Xd\")\n",
    "            print(Xd.shape)\n",
    "            print(Xd.dtype)\n",
    "            \n",
    "            print(\"X\")\n",
    "            print(Xd[idx,:].shape)\n",
    "            \n",
    "            print(\"y\")\n",
    "            print(yd[idx])\n",
    "            print(yd.dtype)\n",
    "            \"\"\"\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, t_loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            print(\"t_loss\")\n",
    "            print(t_loss.shape)\n",
    "            print(t_loss)\n",
    "            \n",
    "            \n",
    "            print(\"corr\")\n",
    "            print(corr)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "            \n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "my_batch_size = 100\n",
    "train_num_epochs = 1\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/gpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,train_num_epochs,my_batch_size,1,train_step,True)\n",
    "        \"\"\"\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,mean_loss,X_val,y_val,1,my_batch_size)\n",
    "        print('Test')\n",
    "        run_model(sess,y_out,mean_loss,X_test,y_test,1,my_batch_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
